{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "authorship_tag": "ABX9TyMJHABcgFacpc4z662qkAz9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/namesarnav/SimMIM/blob/main/ViT_MIM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Masked Image Modeling using ViT-B on a CIFAR-100 dataset"
      ],
      "metadata": {
        "id": "ccmuLCSBKrD2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.datasets import CIFAR100\n",
        "from einops import rearrange, repeat\n",
        "from einops.layers.torch import Rearrange\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "8eWJdFs0RgNo"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set random seeds for reproducibility\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "# Check for GPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Hyperparameters\n",
        "BATCH_SIZE = 2048\n",
        "EPOCHS = 15\n",
        "LEARNING_RATE = 3e-4\n",
        "WEIGHT_DECAY = 0.01\n",
        "IMAGE_SIZE = 32  # CIFAR images are 32x32\n",
        "PATCH_SIZE = 4  # Size of patches for ViT\n",
        "NUM_CLASSES = 100  # CIFAR-100 has 100 classes\n",
        "MASK_RATIO = 0.75  # Portion of patches to mask\n",
        "EMBED_DIM = 384  # Embedding dimension\n",
        "DEPTH = 6  # Number of transformer layers\n",
        "NUM_HEADS = 6  # Number of attention heads\n",
        "MLP_RATIO = 4.0  # Ratio for MLP hidden dim"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I_JR71sBRieq",
        "outputId": "59898810-a8fa-4035-b580-0df4ba7c35fd"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Transformer components\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, dim, hidden_dim, dropout=0.0):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(dim, hidden_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden_dim, dim),\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self, dim, num_heads=8, qkv_bias=False, attn_drop=0., proj_drop=0.):\n",
        "        super().__init__()\n",
        "        self.num_heads = num_heads\n",
        "        head_dim = dim // num_heads\n",
        "        self.scale = head_dim ** -0.5\n",
        "\n",
        "        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
        "        self.attn_drop = nn.Dropout(attn_drop)\n",
        "        self.proj = nn.Linear(dim, dim)\n",
        "        self.proj_drop = nn.Dropout(proj_drop)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, N, C = x.shape\n",
        "        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
        "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
        "\n",
        "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
        "        attn = attn.softmax(dim=-1)\n",
        "        attn = self.attn_drop(attn)\n",
        "\n",
        "        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
        "        x = self.proj(x)\n",
        "        x = self.proj_drop(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class Block(nn.Module):\n",
        "    def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=False, drop=0., attn_drop=0.):\n",
        "        super().__init__()\n",
        "        self.norm1 = nn.LayerNorm(dim)\n",
        "        self.attn = Attention(\n",
        "            dim, num_heads=num_heads, qkv_bias=qkv_bias, attn_drop=attn_drop, proj_drop=drop)\n",
        "        self.norm2 = nn.LayerNorm(dim)\n",
        "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
        "        self.mlp = MLP(dim=dim, hidden_dim=mlp_hidden_dim, dropout=drop)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.attn(self.norm1(x))\n",
        "        x = x + self.mlp(self.norm2(x))\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "yTkTeiHfRmOE"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Masked Image Modeling ViT\n",
        "class MaskedVisionTransformer(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        img_size=IMAGE_SIZE,\n",
        "        patch_size=PATCH_SIZE,\n",
        "        in_chans=3,\n",
        "        embed_dim=EMBED_DIM,\n",
        "        depth=DEPTH,\n",
        "        num_heads=NUM_HEADS,\n",
        "        mlp_ratio=MLP_RATIO,\n",
        "        qkv_bias=True,\n",
        "        drop_rate=0.1,\n",
        "        attn_drop_rate=0.1\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.img_size = img_size\n",
        "        self.patch_size = patch_size\n",
        "        self.in_chans = in_chans\n",
        "        self.embed_dim = embed_dim\n",
        "\n",
        "        # Number of patches\n",
        "        self.num_patches = (img_size // patch_size) ** 2\n",
        "\n",
        "        # Patch embedding\n",
        "        self.patch_embed = nn.Sequential(\n",
        "            Rearrange('b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1=patch_size, p2=patch_size),\n",
        "            nn.Linear(patch_size * patch_size * in_chans, embed_dim),\n",
        "        )\n",
        "\n",
        "        # CLS token and positional embedding\n",
        "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
        "        self.pos_embed = nn.Parameter(torch.zeros(1, self.num_patches + 1, embed_dim))\n",
        "\n",
        "        # Transformer blocks\n",
        "        self.blocks = nn.ModuleList([\n",
        "            Block(\n",
        "                dim=embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias,\n",
        "                drop=drop_rate, attn_drop=attn_drop_rate\n",
        "            )\n",
        "            for _ in range(depth)\n",
        "        ])\n",
        "\n",
        "        # Norm layer\n",
        "        self.norm = nn.LayerNorm(embed_dim)\n",
        "\n",
        "\n",
        "\n",
        "        # Initialize weights\n",
        "        nn.init.trunc_normal_(self.pos_embed, std=0.02)\n",
        "        nn.init.trunc_normal_(self.cls_token, std=0.02)\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "        # Simplified decoder for SimMIM\n",
        "        # In SimMIM, the decoder is intentionally kept very lightweight - just a linear projection\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Linear(embed_dim, patch_size * patch_size * in_chans),\n",
        "            Rearrange('b (h w) (p1 p2 c) -> b c (h p1) (w p2)',\n",
        "                      h=img_size//patch_size, w=img_size//patch_size,\n",
        "                      p1=patch_size, p2=patch_size)\n",
        "        )\n",
        "\n",
        "    def _init_weights(self, m):\n",
        "        if isinstance(m, nn.Linear):\n",
        "            nn.init.trunc_normal_(m.weight, std=0.02)\n",
        "            if m.bias is not None:\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "        elif isinstance(m, nn.LayerNorm):\n",
        "            nn.init.constant_(m.bias, 0)\n",
        "            nn.init.constant_(m.weight, 1.0)\n",
        "\n",
        "    def random_masking(self, x, mask_ratio):\n",
        "        \"\"\"\n",
        "        Perform random masking following SimMIM approach - independent random masking.\n",
        "\n",
        "        Args:\n",
        "            x: [N, L, D], sequence\n",
        "            mask_ratio: percentage of tokens to be masked\n",
        "\n",
        "        Returns:\n",
        "            x_unchanged: [N, L, D], the original sequence (not actually masked)\n",
        "            mask: [N, L], mask -> 0 is keep, 1 is mask (matches SimMIM notation)\n",
        "            ids_restore: None (not needed for SimMIM approach)\n",
        "        \"\"\"\n",
        "        N, L, D = x.shape  # batch, length, dim\n",
        "\n",
        "        # Generate random mask - following SimMIM's approach of independent random masking\n",
        "        # In SimMIM, mask=1 means the token is masked, mask=0 means it's kept\n",
        "        mask = torch.bernoulli(torch.ones(N, L, device=x.device) * mask_ratio)\n",
        "\n",
        "        # In SimMIM, we don't actually need to mask the sequence for the encoder\n",
        "        # The entire sequence is processed, and the loss is only applied on masked tokens\n",
        "\n",
        "        return x, mask, None\n",
        "\n",
        "    def forward_encoder(self, img, mask_ratio):\n",
        "        # Convert image to patches\n",
        "        patches = self.patch_embed(img)  # [B, num_patches, embed_dim]\n",
        "\n",
        "        # Add position embeddings (exclude CLS token position at this point)\n",
        "        patches = patches + self.pos_embed[:, 1:, :]\n",
        "\n",
        "        # Apply masking strategy (following SimMIM, this doesn't actually mask inputs)\n",
        "        patches, mask, _ = self.random_masking(patches, mask_ratio)\n",
        "\n",
        "        # Append CLS token\n",
        "        cls_token = self.cls_token + self.pos_embed[:, :1, :]\n",
        "        cls_tokens = cls_token.expand(patches.shape[0], -1, -1)\n",
        "        x = torch.cat((cls_tokens, patches), dim=1)\n",
        "\n",
        "        # Apply Transformer blocks\n",
        "        for blk in self.blocks:\n",
        "            x = blk(x)\n",
        "\n",
        "        # Apply norm\n",
        "        x = self.norm(x)\n",
        "\n",
        "        return x, mask\n",
        "\n",
        "    def forward_decoder(self, x):\n",
        "        \"\"\"\n",
        "        SimMIM-style lightweight decoder - just project encoded patches back to pixels\n",
        "        \"\"\"\n",
        "        # Exclude CLS token\n",
        "        x = x[:, 1:, :]\n",
        "\n",
        "        # Following SimMIM's lightweight decoder approach\n",
        "        # Just a simple linear projection from embeddings back to pixels\n",
        "        pixels = self.decoder(x)\n",
        "\n",
        "        return pixels\n",
        "\n",
        "    def forward(self, imgs, mask_ratio=MASK_RATIO):\n",
        "        # Encoding with masking strategy (SimMIM style)\n",
        "        latent, mask = self.forward_encoder(imgs, mask_ratio)\n",
        "\n",
        "        # Decoding to reconstruct original image with simplified decoder\n",
        "        pred = self.forward_decoder(latent)\n",
        "\n",
        "        return pred, mask"
      ],
      "metadata": {
        "id": "PtHBfQUwRp21"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset preparation with transformations\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5071, 0.4867, 0.4408), (0.2675, 0.2565, 0.2761))\n",
        "])\n",
        "\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5071, 0.4867, 0.4408), (0.2675, 0.2565, 0.2761))\n",
        "])\n",
        "\n",
        "# Load CIFAR-100 dataset\n",
        "trainset = CIFAR100(root='./data', train=True, download=True, transform=transform_train)\n",
        "trainloader = DataLoader(trainset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
        "\n",
        "testset = CIFAR100(root='./data', train=False, download=True, transform=transform_test)\n",
        "testloader = DataLoader(testset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
        "\n",
        "\n",
        "# Create model\n",
        "model = MaskedVisionTransformer().to(device)\n",
        "\n",
        "# L1 Loss for Masked Image Modeling\n",
        "criterion = nn.L1Loss()\n",
        "\n",
        "# Optimizer with weight decay\n",
        "optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
        "\n",
        "# Learning rate scheduler\n",
        "lr_scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS)"
      ],
      "metadata": {
        "id": "NO3EC0J1RsZN"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training function following SimMIM approach\n",
        "def train_epoch(model, dataloader, criterion, optimizer, device, epoch):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    progress_bar = tqdm(enumerate(dataloader), total=len(dataloader), desc=f\"Epoch {epoch+1}/{EPOCHS}\")\n",
        "\n",
        "    for i, (images, _) in progress_bar:\n",
        "        images = images.to(device)\n",
        "\n",
        "        # Forward pass with masking\n",
        "        reconstructed, mask = model(images)\n",
        "\n",
        "        # Calculate loss only on masked patches - SimMIM style\n",
        "        # In SimMIM, mask=1 means the token is masked\n",
        "        loss = compute_simmim_loss(criterion, reconstructed, images, mask)\n",
        "\n",
        "        # Backward and optimize\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Update statistics\n",
        "        running_loss += loss.item()\n",
        "        progress_bar.set_postfix({\"Loss\": f\"{running_loss/(i+1):.4f}\"})\n",
        "\n",
        "    return running_loss / len(dataloader)\n",
        "\n",
        "# SimMIM loss computation - only on masked patches\n",
        "def compute_simmim_loss(criterion, pred, target, mask):\n",
        "    \"\"\"\n",
        "    Compute L1 loss only on masked patches as in SimMIM\n",
        "\n",
        "    Args:\n",
        "        criterion: loss function (L1Loss)\n",
        "        pred: [B, C, H, W] reconstructed images\n",
        "        target: [B, C, H, W] original images\n",
        "        mask: [B, L] binary mask (1 = masked, 0 = kept)\n",
        "\n",
        "    Returns:\n",
        "        loss: scalar\n",
        "    \"\"\"\n",
        "    # Reshape images to match mask dimensions\n",
        "    B, C, H, W = pred.shape\n",
        "    patch_size = PATCH_SIZE\n",
        "\n",
        "    # Reshape predictions and targets to patch-level\n",
        "    # [B, C, H, W] -> [B, L, C*P*P] where L is number of patches, P is patch size\n",
        "    pred = rearrange(pred, 'b c (h p1) (w p2) -> b (h w) (c p1 p2)', p1=patch_size, p2=patch_size)\n",
        "    target = rearrange(target, 'b c (h p1) (w p2) -> b (h w) (c p1 p2)', p1=patch_size, p2=patch_size)\n",
        "\n",
        "    # Apply mask: only compute loss on masked patches (where mask == 1)\n",
        "    loss = criterion(pred[mask.bool()], target[mask.bool()])\n",
        "\n",
        "    return loss\n",
        "\n",
        "\n",
        "# Function to visualize original and reconstructed images with SimMIM masking\n",
        "def visualize_reconstruction(model, dataloader, device, num_images=5):\n",
        "    model.eval()\n",
        "    images, _ = next(iter(dataloader))\n",
        "    images = images[:num_images].to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        reconstructed, mask = model(images)\n",
        "\n",
        "    # Convert mask for visualization\n",
        "    # In SimMIM, mask=1 means the token is masked, so we need to reshape it for display\n",
        "    mask_vis = mask.reshape(mask.shape[0], 8, 8)  # Assuming 32x32 image with 4x4 patches\n",
        "    mask_vis = mask_vis.unsqueeze(1).repeat(1, 3, 1, 1)  # [B, 3, 8, 8]\n",
        "    # Upsample mask to match image dimensions\n",
        "    mask_vis = F.interpolate(mask_vis.float(), size=(32, 32), mode='nearest')\n",
        "\n",
        "    # Convert to CPU for plotting\n",
        "    images = images.cpu()\n",
        "    reconstructed = reconstructed.cpu()\n",
        "    mask_vis = mask_vis.cpu()\n",
        "\n",
        "    # Denormalize images\n",
        "    mean = torch.tensor([0.5071, 0.4867, 0.4408]).view(1, 3, 1, 1)\n",
        "    std = torch.tensor([0.2675, 0.2565, 0.2761]).view(1, 3, 1, 1)\n",
        "\n",
        "    images = images * std + mean\n",
        "    reconstructed = reconstructed * std + mean\n",
        "\n",
        "    # Clip values to [0, 1] range\n",
        "    images = torch.clamp(images, 0, 1)\n",
        "    reconstructed = torch.clamp(reconstructed, 0, 1)\n",
        "\n",
        "    # Create masked image by combining original and reconstructed based on mask\n",
        "    masked_imgs = images.clone()\n",
        "    for i in range(num_images):\n",
        "        # Apply mask - use reconstructed image where mask=1 (masked regions)\n",
        "        mask_image = mask_vis[i]\n",
        "        masked_imgs[i] = images[i] * (1 - mask_image) + reconstructed[i] * mask_image\n",
        "\n",
        "    # Plot\n",
        "    fig, axs = plt.subplots(3, num_images, figsize=(15, 8))\n",
        "\n",
        "    for i in range(num_images):\n",
        "        # Original\n",
        "        axs[0, i].imshow(images[i].permute(1, 2, 0))\n",
        "        axs[0, i].set_title(\"Original\")\n",
        "        axs[0, i].axis('off')\n",
        "\n",
        "        # Mask visualization (white = masked in SimMIM)\n",
        "        axs[1, i].imshow(mask_vis[i].permute(1, 2, 0), cmap='gray')\n",
        "        axs[1, i].set_title(\"Mask (white=masked)\")\n",
        "        axs[1, i].axis('off')\n",
        "\n",
        "        # Reconstructed\n",
        "        axs[2, i].imshow(masked_imgs[i].permute(1, 2, 0))\n",
        "        axs[2, i].set_title(\"Masked + Reconstructed\")\n",
        "        axs[2, i].axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f\"simmim_reconstruction_example.png\")\n",
        "    plt.close()\n",
        "\n"
      ],
      "metadata": {
        "id": "hvQ8thr1Rxds"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training loop\n",
        "def train_model(model, trainloader, testloader, criterion, optimizer, lr_scheduler, device, epochs):\n",
        "    best_loss = float('inf')\n",
        "    train_losses = []\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        # Train for one epoch\n",
        "        train_loss = train_epoch(model, trainloader, criterion, optimizer, device, epoch)\n",
        "        train_losses.append(train_loss)\n",
        "\n",
        "        # Update learning rate\n",
        "        lr_scheduler.step()\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {train_loss:.4f}\")\n",
        "\n",
        "        # Save checkpoint if best model\n",
        "        if train_loss < best_loss:\n",
        "            best_loss = train_loss\n",
        "            torch.save({\n",
        "                'epoch': epoch,\n",
        "                'model_state_dict': model.state_dict(),\n",
        "                'optimizer_state_dict': optimizer.state_dict(),\n",
        "                'loss': best_loss,\n",
        "            }, 'best_vit_mim_model.pth')\n",
        "            print(f\"Checkpoint saved (Loss: {best_loss:.4f})\")\n",
        "\n",
        "        # Visualize reconstruction every 5 epochs\n",
        "        if (epoch + 1) % 5 == 0:\n",
        "            visualize_reconstruction(model, testloader, device)\n",
        "\n",
        "    # Plot training loss\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.plot(train_losses, label='Training Loss')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title('Training Loss Over Time')\n",
        "    plt.legend()\n",
        "    plt.savefig('training_loss.png')\n",
        "    plt.close()\n",
        "\n",
        "    return train_losses\n",
        "\n",
        "\n",
        "# Main execution\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"Starting training Vision Transformer with Masked Image Modeling on CIFAR-100...\")\n",
        "\n",
        "    # Train the model\n",
        "    losses = train_model(\n",
        "        model=model,\n",
        "        trainloader=trainloader,\n",
        "        testloader=testloader,\n",
        "        criterion=criterion,\n",
        "        optimizer=optimizer,\n",
        "        lr_scheduler=lr_scheduler,\n",
        "        device=device,\n",
        "        epochs=EPOCHS\n",
        "    )\n",
        "\n",
        "    print(\"Training complete!\")\n",
        "\n",
        "    # Load best model for final visualization\n",
        "    checkpoint = torch.load('best_vit_mim_model.pth')\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "\n",
        "    # Final reconstruction visualization\n",
        "    print(\"Creating final reconstruction visualization...\")\n",
        "    visualize_reconstruction(model, testloader, device, num_images=8)\n",
        "    print(\"Visualization saved as 'reconstruction_example.png'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JD7h9XKyRU2A",
        "outputId": "a5b9e06e-87fa-42d0-d3b2-02e29256136f"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting training Vision Transformer with Masked Image Modeling on CIFAR-100...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/15: 100%|██████████| 25/25 [00:17<00:00,  1.42it/s, Loss=0.3742]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/15, Train Loss: 0.3742\n",
            "Checkpoint saved (Loss: 0.3742)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/15: 100%|██████████| 25/25 [00:17<00:00,  1.42it/s, Loss=0.1901]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/15, Train Loss: 0.1901\n",
            "Checkpoint saved (Loss: 0.1901)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/15: 100%|██████████| 25/25 [00:17<00:00,  1.42it/s, Loss=0.1484]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3/15, Train Loss: 0.1484\n",
            "Checkpoint saved (Loss: 0.1484)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4/15: 100%|██████████| 25/25 [00:17<00:00,  1.42it/s, Loss=0.1335]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4/15, Train Loss: 0.1335\n",
            "Checkpoint saved (Loss: 0.1335)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5/15: 100%|██████████| 25/25 [00:17<00:00,  1.42it/s, Loss=0.1162]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5/15, Train Loss: 0.1162\n",
            "Checkpoint saved (Loss: 0.1162)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch 6/15:   0%|          | 0/25 [00:00<?, ?it/s]Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7feca9196a20>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1618, in __del__\n",
            "    self._shutdown_workers()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1601, in _shutdown_workers\n",
            "    if w.is_alive():\n",
            "       ^^Exception ignored in: ^^<function _MultiProcessingDataLoaderIter.__del__ at 0x7feca9196a20>^\n",
            "^Traceback (most recent call last):\n",
            "^  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1618, in __del__\n",
            "^^    ^self._shutdown_workers()^\n",
            "^  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1601, in _shutdown_workers\n",
            "\n",
            "      File \"/usr/lib/python3.11/multiprocessing/process.py\", line 160, in is_alive\n",
            "if w.is_alive():    \n",
            "assert self._parent_pid == os.getpid(), 'can only test a child process' \n",
            "            ^ ^ ^ ^ ^ ^^^^^^^^^^^^^^\n",
            "^  File \"/usr/lib/python3.11/multiprocessing/process.py\", line 160, in is_alive\n",
            "^    ^assert self._parent_pid == os.getpid(), 'can only test a child process'^\n",
            "^ ^ ^ ^ ^ ^^ ^ ^ ^ ^ ^ ^^^^^^^^^^^^^^^^\n",
            "^AssertionError^: ^can only test a child process^\n",
            "^^^^^Exception ignored in: ^<function _MultiProcessingDataLoaderIter.__del__ at 0x7feca9196a20>^\n",
            "^Traceback (most recent call last):\n",
            "^  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1618, in __del__\n",
            "^    ^^self._shutdown_workers()\n",
            "^  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1601, in _shutdown_workers\n",
            "^^    ^if w.is_alive():^\n",
            "^ ^ \n",
            " AssertionError :  can only test a child process\n",
            "  ^^^Exception ignored in: ^<function _MultiProcessingDataLoaderIter.__del__ at 0x7feca9196a20>^\n",
            "^Traceback (most recent call last):\n",
            "^  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1618, in __del__\n",
            "^    ^self._shutdown_workers()^\n",
            "^  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1601, in _shutdown_workers\n",
            "^\n",
            "      File \"/usr/lib/python3.11/multiprocessing/process.py\", line 160, in is_alive\n",
            "if w.is_alive():    \n",
            "assert self._parent_pid == os.getpid(), 'can only test a child process'  \n",
            "          ^ ^ ^ ^ ^ ^ ^^^^^^^^^^^^\n",
            "^  File \"/usr/lib/python3.11/multiprocessing/process.py\", line 160, in is_alive\n",
            "^    ^assert self._parent_pid == os.getpid(), 'can only test a child process'^\n",
            "^ ^ ^^ ^ ^  ^ ^ ^ ^ ^ ^^^^^^^^^^^^^^^^^^^^\n",
            "^AssertionError^: ^can only test a child process^\n",
            "^^^^^Exception ignored in: ^<function _MultiProcessingDataLoaderIter.__del__ at 0x7feca9196a20>^\n",
            "^Traceback (most recent call last):\n",
            "^  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1618, in __del__\n",
            "^^    ^self._shutdown_workers()^\n",
            "^  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1601, in _shutdown_workers\n",
            "^    ^if w.is_alive():^\n",
            "\n",
            " AssertionError :  can only test a child process \n",
            "   ^^Exception ignored in: ^<function _MultiProcessingDataLoaderIter.__del__ at 0x7feca9196a20>^\n",
            "^Traceback (most recent call last):\n",
            "^  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1618, in __del__\n",
            "^    ^self._shutdown_workers()^\n",
            "^  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1601, in _shutdown_workers\n",
            "^^    \n",
            "if w.is_alive():  File \"/usr/lib/python3.11/multiprocessing/process.py\", line 160, in is_alive\n",
            "\n",
            "     assert self._parent_pid == os.getpid(), 'can only test a child process'  \n",
            "        ^ ^ ^ ^ ^ ^ ^ ^^^^^^^^^^\n",
            "^  File \"/usr/lib/python3.11/multiprocessing/process.py\", line 160, in is_alive\n",
            "^    ^assert self._parent_pid == os.getpid(), 'can only test a child process'^\n",
            "^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^^^^^^^^^^^^^^^^^^^^^^\n",
            "^AssertionError^: ^can only test a child process^\n",
            "^^^^^^^^^^^^^^^^\n",
            "AssertionError: can only test a child process\n",
            "Epoch 6/15: 100%|██████████| 25/25 [00:18<00:00,  1.38it/s, Loss=0.1079]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6/15, Train Loss: 0.1079\n",
            "Checkpoint saved (Loss: 0.1079)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch 7/15:   0%|          | 0/25 [00:00<?, ?it/s]Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7feca9196a20>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1618, in __del__\n",
            "    self._shutdown_workers()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1601, in _shutdown_workers\n",
            "    if w.is_alive():\n",
            "       ^^^^^^^Exception ignored in: ^^<function _MultiProcessingDataLoaderIter.__del__ at 0x7feca9196a20>\n",
            "^Traceback (most recent call last):\n",
            "^  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1618, in __del__\n",
            "^\n",
            "      File \"/usr/lib/python3.11/multiprocessing/process.py\", line 160, in is_alive\n",
            "self._shutdown_workers()    \n",
            "assert self._parent_pid == os.getpid(), 'can only test a child process'  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1601, in _shutdown_workers\n",
            "\n",
            "      if w.is_alive(): \n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "^  File \"/usr/lib/python3.11/multiprocessing/process.py\", line 160, in is_alive\n",
            "^    ^assert self._parent_pid == os.getpid(), 'can only test a child process'^\n",
            "^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^^^^^^^^\n",
            "^AssertionError^: ^can only test a child process^\n",
            "^^^^^Exception ignored in: ^^<function _MultiProcessingDataLoaderIter.__del__ at 0x7feca9196a20>^\n",
            "^Traceback (most recent call last):\n",
            "^  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1618, in __del__\n",
            "^^    ^self._shutdown_workers()^\n",
            "^  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1601, in _shutdown_workers\n",
            "^    ^if w.is_alive():^\n",
            "^ ^ ^ ^ ^ \n",
            " AssertionError : ^can only test a child process^\n",
            "^^^^^Exception ignored in: ^<function _MultiProcessingDataLoaderIter.__del__ at 0x7feca9196a20>^\n",
            "Traceback (most recent call last):\n",
            "^^  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1618, in __del__\n",
            "^    \n",
            "self._shutdown_workers()  File \"/usr/lib/python3.11/multiprocessing/process.py\", line 160, in is_alive\n",
            "\n",
            "      File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1601, in _shutdown_workers\n",
            "assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
            "    if w.is_alive(): \n",
            "               ^ ^ ^^^^^^^^^^^^^^^^^^^^^\n",
            "^  File \"/usr/lib/python3.11/multiprocessing/process.py\", line 160, in is_alive\n",
            "^    ^assert self._parent_pid == os.getpid(), 'can only test a child process'^\n",
            "^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^^^^^^^^^^\n",
            "^AssertionError^: ^^can only test a child process\n",
            "^^^^^Exception ignored in: ^<function _MultiProcessingDataLoaderIter.__del__ at 0x7feca9196a20>^\n",
            "^Traceback (most recent call last):\n",
            "^  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1618, in __del__\n",
            "^^    ^self._shutdown_workers()^\n",
            "^  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1601, in _shutdown_workers\n",
            "^    ^if w.is_alive():^\n",
            "^ ^ ^ ^ ^ \n",
            "  AssertionError^: can only test a child process^^\n",
            "^^^^^Exception ignored in: ^<function _MultiProcessingDataLoaderIter.__del__ at 0x7feca9196a20>^\n",
            "^Traceback (most recent call last):\n",
            "^  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1618, in __del__\n",
            "\n",
            "      File \"/usr/lib/python3.11/multiprocessing/process.py\", line 160, in is_alive\n",
            "self._shutdown_workers()    \n",
            "assert self._parent_pid == os.getpid(), 'can only test a child process'  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1601, in _shutdown_workers\n",
            "\n",
            "      if w.is_alive(): \n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "^  File \"/usr/lib/python3.11/multiprocessing/process.py\", line 160, in is_alive\n",
            "^^    ^assert self._parent_pid == os.getpid(), 'can only test a child process'^\n",
            "^ ^ ^ ^ ^^ ^ ^ ^ ^ ^ ^ ^^^^\n",
            "^AssertionError^: ^can only test a child process^\n",
            "^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "AssertionError: can only test a child process\n",
            "Epoch 7/15: 100%|██████████| 25/25 [00:18<00:00,  1.39it/s, Loss=0.1020]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7/15, Train Loss: 0.1020\n",
            "Checkpoint saved (Loss: 0.1020)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch 8/15:   0%|          | 0/25 [00:00<?, ?it/s]Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7feca9196a20>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1618, in __del__\n",
            "    self._shutdown_workers()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1601, in _shutdown_workers\n",
            "    if w.is_alive():\n",
            "       ^^^^^^^^^^^^Exception ignored in: \n",
            "  File \"/usr/lib/python3.11/multiprocessing/process.py\", line 160, in is_alive\n",
            "<function _MultiProcessingDataLoaderIter.__del__ at 0x7feca9196a20>    \n",
            "assert self._parent_pid == os.getpid(), 'can only test a child process'Traceback (most recent call last):\n",
            "\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1618, in __del__\n",
            "      self._shutdown_workers() \n",
            "   File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1601, in _shutdown_workers\n",
            "      if w.is_alive(): \n",
            "        ^ ^ ^ ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "^  File \"/usr/lib/python3.11/multiprocessing/process.py\", line 160, in is_alive\n",
            "^^    assert self._parent_pid == os.getpid(), 'can only test a child process'^\n",
            "^ ^ ^ ^ ^ ^^ ^ ^ ^ ^ \n",
            " AssertionError^: ^can only test a child process^\n",
            "^^^^^Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7feca9196a20>^\n",
            "^Traceback (most recent call last):\n",
            "^  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1618, in __del__\n",
            "^^    ^self._shutdown_workers()^\n",
            "^  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1601, in _shutdown_workers\n",
            "^    ^if w.is_alive():^\n",
            "^ ^ ^ ^ ^ ^ ^ ^^^^^^^^^^\n",
            "^AssertionError^: ^can only test a child process^\n",
            "^^^\n",
            "Exception ignored in:   File \"/usr/lib/python3.11/multiprocessing/process.py\", line 160, in is_alive\n",
            "<function _MultiProcessingDataLoaderIter.__del__ at 0x7feca9196a20>    \n",
            "assert self._parent_pid == os.getpid(), 'can only test a child process'Traceback (most recent call last):\n",
            "\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1618, in __del__\n",
            "      self._shutdown_workers() \n",
            "   File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1601, in _shutdown_workers\n",
            "       if w.is_alive(): \n",
            "      ^ ^ ^ ^^ ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "^  File \"/usr/lib/python3.11/multiprocessing/process.py\", line 160, in is_alive\n",
            "^    ^^assert self._parent_pid == os.getpid(), 'can only test a child process'^\n",
            "^ ^ ^ ^ ^ ^ ^ ^ ^  \n",
            "AssertionError : ^can only test a child process^\n",
            "^^^^^Exception ignored in: ^<function _MultiProcessingDataLoaderIter.__del__ at 0x7feca9196a20>^\n",
            "^Traceback (most recent call last):\n",
            "^  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1618, in __del__\n",
            "^    ^self._shutdown_workers()^\n",
            "^  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1601, in _shutdown_workers\n",
            "^^    ^if w.is_alive():^\n",
            " ^^ ^  ^^ ^ ^ ^^^^^^^^^^\n",
            "^^AssertionError^: ^can only test a child process^\n",
            "^^\n",
            "  File \"/usr/lib/python3.11/multiprocessing/process.py\", line 160, in is_alive\n",
            "Exception ignored in:     <function _MultiProcessingDataLoaderIter.__del__ at 0x7feca9196a20>assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
            "\n",
            "Traceback (most recent call last):\n",
            "   File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1618, in __del__\n",
            "       self._shutdown_workers() \n",
            "   File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1601, in _shutdown_workers\n",
            "      if w.is_alive(): \n",
            "    ^ ^ ^ ^ ^ ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "^  File \"/usr/lib/python3.11/multiprocessing/process.py\", line 160, in is_alive\n",
            "^    assert self._parent_pid == os.getpid(), 'can only test a child process'^\n",
            "^ ^ ^ ^ ^ ^ ^ ^^ ^ \n",
            " AssertionError : can only test a child process^\n",
            "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "AssertionError: can only test a child process\n",
            "Epoch 8/15: 100%|██████████| 25/25 [00:18<00:00,  1.38it/s, Loss=0.0967]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8/15, Train Loss: 0.0967\n",
            "Checkpoint saved (Loss: 0.0967)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 9/15:   4%|▍         | 1/25 [00:01<00:36,  1.53s/it, Loss=0.0929]Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7feca9196a20>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1618, in __del__\n",
            "    self._shutdown_workers()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1601, in _shutdown_workers\n",
            "    if w.is_alive():\n",
            "   Exception ignored in:   <function _MultiProcessingDataLoaderIter.__del__ at 0x7feca9196a20> \n",
            " Traceback (most recent call last):\n",
            "^  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1618, in __del__\n",
            "^^    ^self._shutdown_workers()^\n",
            "^  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1601, in _shutdown_workers\n",
            "^    ^^if w.is_alive():^\n",
            "^ ^ \n",
            "    File \"/usr/lib/python3.11/multiprocessing/process.py\", line 160, in is_alive\n",
            "      assert self._parent_pid == os.getpid(), 'can only test a child process' \n",
            "^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^^\n",
            "^  File \"/usr/lib/python3.11/multiprocessing/process.py\", line 160, in is_alive\n",
            "^    ^assert self._parent_pid == os.getpid(), 'can only test a child process'^\n",
            "^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "^AssertionError^: ^can only test a child process^\n",
            "^^^^Exception ignored in: ^^<function _MultiProcessingDataLoaderIter.__del__ at 0x7feca9196a20>^\n",
            "Traceback (most recent call last):\n",
            "^  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1618, in __del__\n",
            "^^    ^self._shutdown_workers()^\n",
            "^  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1601, in _shutdown_workers\n",
            "^\n",
            "    AssertionErrorif w.is_alive():: \n",
            "can only test a child process \n",
            "      Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7feca9196a20>^\n",
            "^Traceback (most recent call last):\n",
            "^  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1618, in __del__\n",
            "^^    ^self._shutdown_workers()^\n",
            "^  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1601, in _shutdown_workers\n",
            "^    ^if w.is_alive():^\n",
            "^ \n",
            "   File \"/usr/lib/python3.11/multiprocessing/process.py\", line 160, in is_alive\n",
            "      assert self._parent_pid == os.getpid(), 'can only test a child process' \n",
            "     ^ ^ ^ ^^ ^ ^ ^  ^^^^^^^^\n",
            "^  File \"/usr/lib/python3.11/multiprocessing/process.py\", line 160, in is_alive\n",
            "^    ^assert self._parent_pid == os.getpid(), 'can only test a child process'^\n",
            "^ ^ ^ ^ ^ ^^ ^ ^ ^ ^ ^ ^^^^^^^^^^^^^^^^^^^^^^\n",
            "^AssertionError^: ^can only test a child process^\n",
            "^^^^^Exception ignored in: ^<function _MultiProcessingDataLoaderIter.__del__ at 0x7feca9196a20>^\n",
            "^Traceback (most recent call last):\n",
            "^  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1618, in __del__\n",
            "^^    ^self._shutdown_workers()^\n",
            "^  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1601, in _shutdown_workers\n",
            "^^    \n",
            "if w.is_alive():AssertionError\n",
            ":  can only test a child process \n",
            "    Exception ignored in:  <function _MultiProcessingDataLoaderIter.__del__ at 0x7feca9196a20>^\n",
            "^Traceback (most recent call last):\n",
            "^  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1618, in __del__\n",
            "^    ^self._shutdown_workers()^\n",
            "^  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1601, in _shutdown_workers\n",
            "^    ^if w.is_alive():^\n",
            "^ ^ \n",
            "   File \"/usr/lib/python3.11/multiprocessing/process.py\", line 160, in is_alive\n",
            "      assert self._parent_pid == os.getpid(), 'can only test a child process'  \n",
            "^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^^\n",
            "^  File \"/usr/lib/python3.11/multiprocessing/process.py\", line 160, in is_alive\n",
            "^    ^assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
            "^^ ^ ^ ^ ^ ^ ^ ^^ ^ ^ ^ ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "^AssertionError^: ^can only test a child process^\n",
            "^^^^^^^^^^^^^\n",
            "AssertionError: can only test a child process\n",
            "Epoch 9/15: 100%|██████████| 25/25 [00:17<00:00,  1.42it/s, Loss=0.0914]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 9/15, Train Loss: 0.0914\n",
            "Checkpoint saved (Loss: 0.0914)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 10/15:   4%|▍         | 1/25 [00:01<00:36,  1.52s/it, Loss=0.0892]Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7feca9196a20>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1618, in __del__\n",
            "    self._shutdown_workers()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1601, in _shutdown_workers\n",
            "    if w.is_alive():\n",
            "     Exception ignored in:  <function _MultiProcessingDataLoaderIter.__del__ at 0x7feca9196a20> ^\n",
            "^Traceback (most recent call last):\n",
            "^  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1618, in __del__\n",
            "^^    ^self._shutdown_workers()^\n",
            "^  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1601, in _shutdown_workers\n",
            "^    ^^if w.is_alive():^\n",
            "\n",
            "   File \"/usr/lib/python3.11/multiprocessing/process.py\", line 160, in is_alive\n",
            "       assert self._parent_pid == os.getpid(), 'can only test a child process' \n",
            "    ^ ^  ^ ^ ^ ^ ^ ^ ^^^^^^^^\n",
            "^  File \"/usr/lib/python3.11/multiprocessing/process.py\", line 160, in is_alive\n",
            "^    ^assert self._parent_pid == os.getpid(), 'can only test a child process'^\n",
            "^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "^AssertionError^: ^can only test a child process^\n",
            "^^^^Exception ignored in: ^<function _MultiProcessingDataLoaderIter.__del__ at 0x7feca9196a20>^\n",
            "^^Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1618, in __del__\n",
            "^^    ^self._shutdown_workers()^\n",
            "^  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1601, in _shutdown_workers\n",
            "^^    \n",
            "if w.is_alive():\n",
            "AssertionError:  can only test a child process \n",
            "     Exception ignored in: ^<function _MultiProcessingDataLoaderIter.__del__ at 0x7feca9196a20>^\n",
            "^Traceback (most recent call last):\n",
            "^  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1618, in __del__\n",
            "^    ^self._shutdown_workers()^\n",
            "^  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1601, in _shutdown_workers\n",
            "^^    if w.is_alive():^^\n",
            " \n",
            "   File \"/usr/lib/python3.11/multiprocessing/process.py\", line 160, in is_alive\n",
            "      assert self._parent_pid == os.getpid(), 'can only test a child process' \n",
            "    ^ ^ ^ ^^ ^ ^  ^ ^^ ^^^^^\n",
            "^  File \"/usr/lib/python3.11/multiprocessing/process.py\", line 160, in is_alive\n",
            "^    ^assert self._parent_pid == os.getpid(), 'can only test a child process'^\n",
            "^^ ^ ^ ^ ^ ^ ^ ^ ^  ^ ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "^AssertionError^^: ^can only test a child process\n",
            "^^^^^Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7feca9196a20>^\n",
            "^Traceback (most recent call last):\n",
            "^  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1618, in __del__\n",
            "^^    ^self._shutdown_workers()^\n",
            "^^  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1601, in _shutdown_workers\n",
            "^    \n",
            "if w.is_alive():AssertionError\n",
            ":  can only test a child process \n",
            "    Exception ignored in:  <function _MultiProcessingDataLoaderIter.__del__ at 0x7feca9196a20>^\n",
            "^Traceback (most recent call last):\n",
            "^^  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1618, in __del__\n",
            "^    ^self._shutdown_workers()^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1601, in _shutdown_workers\n",
            "^    ^if w.is_alive():^\n",
            "^ ^ \n",
            "   File \"/usr/lib/python3.11/multiprocessing/process.py\", line 160, in is_alive\n",
            "      assert self._parent_pid == os.getpid(), 'can only test a child process' \n",
            "  ^ ^ ^ ^ ^ ^ ^ ^ ^^ ^ ^^^\n",
            "  File \"/usr/lib/python3.11/multiprocessing/process.py\", line 160, in is_alive\n",
            "^    ^assert self._parent_pid == os.getpid(), 'can only test a child process'^^\n",
            "^  ^ ^ ^^  ^ ^ ^ ^ ^ ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "^AssertionError^: ^can only test a child process^\n",
            "^^^^^^^^^^^^^\n",
            "AssertionError: can only test a child process\n",
            "Epoch 10/15: 100%|██████████| 25/25 [00:17<00:00,  1.42it/s, Loss=0.0884]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10/15, Train Loss: 0.0884\n",
            "Checkpoint saved (Loss: 0.0884)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 11/15: 100%|██████████| 25/25 [00:17<00:00,  1.42it/s, Loss=0.0861]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 11/15, Train Loss: 0.0861\n",
            "Checkpoint saved (Loss: 0.0861)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 12/15: 100%|██████████| 25/25 [00:17<00:00,  1.42it/s, Loss=0.0846]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 12/15, Train Loss: 0.0846\n",
            "Checkpoint saved (Loss: 0.0846)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 13/15: 100%|██████████| 25/25 [00:17<00:00,  1.42it/s, Loss=0.0837]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 13/15, Train Loss: 0.0837\n",
            "Checkpoint saved (Loss: 0.0837)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 14/15: 100%|██████████| 25/25 [00:17<00:00,  1.42it/s, Loss=0.0831]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 14/15, Train Loss: 0.0831\n",
            "Checkpoint saved (Loss: 0.0831)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 15/15: 100%|██████████| 25/25 [00:17<00:00,  1.42it/s, Loss=0.0828]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 15/15, Train Loss: 0.0828\n",
            "Checkpoint saved (Loss: 0.0828)\n",
            "Training complete!\n",
            "Creating final reconstruction visualization...\n",
            "Visualization saved as 'reconstruction_example.png'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Evaluation and Visualization"
      ],
      "metadata": {
        "id": "bfNcYOtoWuWO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the best model\n",
        "def load_best_model(model, checkpoint_path='best_vit_mim_model.pth'):\n",
        "    checkpoint = torch.load(checkpoint_path)\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    print(f\"Loaded model from epoch {checkpoint['epoch']} with loss: {checkpoint['loss']:.4f}\")\n",
        "    return model, checkpoint['epoch'], checkpoint['loss']\n",
        "\n",
        "# Visualize training loss (enhanced version)\n",
        "def plot_training_loss(losses, save_path='training_loss_detailed.png'):\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    epochs = range(1, len(losses) + 1)\n",
        "\n",
        "    plt.plot(epochs, losses, 'b-', linewidth=2, markersize=8, label='Training Loss')\n",
        "\n",
        "    # Add moving average for trend visualization\n",
        "    window_size = min(5, len(losses))\n",
        "    if window_size > 1:\n",
        "        moving_avg = np.convolve(losses, np.ones(window_size)/window_size, mode='valid')\n",
        "        plt.plot(range(window_size, len(losses) + 1), moving_avg, 'r--', linewidth=2, label=f'{window_size}-epoch Moving Avg')\n",
        "\n",
        "    plt.grid(True, linestyle='--', alpha=0.7)\n",
        "    plt.xlabel('Epochs', fontsize=12)\n",
        "    plt.ylabel('L1 Loss', fontsize=12)\n",
        "    plt.title('Training Loss Over Time', fontsize=14)\n",
        "    plt.legend(fontsize=10)\n",
        "\n",
        "    # Annotate min loss\n",
        "    min_loss_epoch = np.argmin(losses) + 1\n",
        "    min_loss = min(losses)\n",
        "    plt.annotate(f'Min: {min_loss:.4f}',\n",
        "                xy=(min_loss_epoch, min_loss),\n",
        "                xytext=(min_loss_epoch, min_loss*1.2),\n",
        "                arrowprops=dict(facecolor='black', shrink=0.05, width=1.5),\n",
        "                fontsize=10)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(save_path, dpi=300)\n",
        "    plt.close()\n",
        "    print(f\"Loss plot saved to {save_path}\")\n",
        "\n",
        "# Enhanced visualization of reconstructions\n",
        "def visualize_reconstruction_enhanced(model, dataloader, device, num_images=8, save_path='reconstruction_detailed.png'):\n",
        "    model.eval()\n",
        "    # Get a batch of images\n",
        "    images, labels = next(iter(dataloader))\n",
        "    images = images[:num_images].to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        reconstructed, mask = model(images)\n",
        "\n",
        "    # Reshape mask for visualization\n",
        "    mask_vis = mask.unsqueeze(-1).repeat(1, 1, 16).reshape(mask.shape[0], 32, 32).unsqueeze(1)\n",
        "    mask_vis = mask_vis.repeat(1, 3, 1, 1)\n",
        "\n",
        "    # Convert to CPU\n",
        "    images = images.cpu()\n",
        "    reconstructed = reconstructed.cpu()\n",
        "    mask_vis = mask_vis.cpu()\n",
        "\n",
        "    # Denormalize images\n",
        "    mean = torch.tensor([0.5071, 0.4867, 0.4408]).view(1, 3, 1, 1)\n",
        "    std = torch.tensor([0.2675, 0.2565, 0.2761]).view(1, 3, 1, 1)\n",
        "\n",
        "    images = images * std + mean\n",
        "    reconstructed = reconstructed * std + mean\n",
        "\n",
        "    # Clip values to [0, 1] range\n",
        "    images = torch.clamp(images, 0, 1)\n",
        "    reconstructed = torch.clamp(reconstructed, 0, 1)\n",
        "\n",
        "    # Create \"masked images\" for better visualization\n",
        "    masked_imgs = images.clone()\n",
        "    for i in range(num_images):\n",
        "        # Apply mask (set masked regions to gray)\n",
        "        mask_regions = (mask_vis[i] > 0.5)\n",
        "        masked_imgs[i][mask_regions] = 0.5  # Gray color for masked regions\n",
        "\n",
        "    # Create a better visualization with 4 rows\n",
        "    fig, axs = plt.subplots(4, num_images, figsize=(num_images*2, 8))\n",
        "\n",
        "    for i in range(num_images):\n",
        "        # Original\n",
        "        axs[0, i].imshow(images[i].permute(1, 2, 0))\n",
        "        axs[0, i].set_title(\"Original\", fontsize=10)\n",
        "        axs[0, i].axis('off')\n",
        "\n",
        "        # Mask (white = masked)\n",
        "        axs[1, i].imshow(1 - mask_vis[i].permute(1, 2, 0), cmap='gray')\n",
        "        axs[1, i].set_title(\"Mask\", fontsize=10)\n",
        "        axs[1, i].axis('off')\n",
        "\n",
        "        # Masked image\n",
        "        axs[2, i].imshow(masked_imgs[i].permute(1, 2, 0))\n",
        "        axs[2, i].set_title(\"Masked Image\", fontsize=10)\n",
        "        axs[2, i].axis('off')\n",
        "\n",
        "        # Reconstructed\n",
        "        axs[3, i].imshow(reconstructed[i].permute(1, 2, 0))\n",
        "        axs[3, i].set_title(\"Reconstructed\", fontsize=10)\n",
        "        axs[3, i].axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(save_path, dpi=300)\n",
        "    plt.close()\n",
        "    print(f\"Reconstruction visualization saved to {save_path}\")\n",
        "\n",
        "    # Calculate and return reconstruction error\n",
        "    mse_loss = nn.MSELoss()(reconstructed, images)\n",
        "    l1_loss = nn.L1Loss()(reconstructed, images)\n",
        "\n",
        "    # Calculate PSNR\n",
        "    mse = torch.mean((reconstructed - images) ** 2)\n",
        "    psnr = 20 * torch.log10(1.0 / torch.sqrt(mse))\n",
        "\n",
        "    print(f\"Reconstruction metrics on sample:\")\n",
        "    print(f\"MSE: {mse_loss.item():.4f}\")\n",
        "    print(f\"L1: {l1_loss.item():.4f}\")\n",
        "    print(f\"PSNR: {psnr.item():.2f} dB\")\n",
        "\n",
        "    return mse_loss.item(), l1_loss.item(), psnr.item()\n",
        "\n",
        "# Visualize learned embeddings\n",
        "def visualize_embeddings(model, dataloader, device, num_samples=500, save_path='embeddings_visualization.png'):\n",
        "    model.eval()\n",
        "\n",
        "    # Store embeddings and labels\n",
        "    all_embeddings = []\n",
        "    all_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in dataloader:\n",
        "            if len(all_embeddings) >= num_samples:\n",
        "                break\n",
        "\n",
        "            images = images.to(device)\n",
        "            batch_size = images.shape[0]\n",
        "\n",
        "            # Get embeddings (CLS token after encoder)\n",
        "            latent, _, _ = model.forward_encoder(images, mask_ratio=0)  # No masking for this visualization\n",
        "            cls_token = latent[:, 0]  # CLS token\n",
        "\n",
        "            all_embeddings.append(cls_token.cpu())\n",
        "            all_labels.append(labels)\n",
        "\n",
        "            if len(all_embeddings) * batch_size >= num_samples:\n",
        "                break\n",
        "\n",
        "    # Concatenate all collected embeddings and labels\n",
        "    embeddings = torch.cat(all_embeddings, dim=0)[:num_samples].numpy()\n",
        "    labels = torch.cat(all_labels, dim=0)[:num_samples].numpy()\n",
        "\n",
        "    # Apply dimensionality reduction with PCA first (for speed)\n",
        "    pca = PCA(n_components=50)\n",
        "    embeddings_pca = pca.fit_transform(embeddings)\n",
        "\n",
        "    # Apply t-SNE\n",
        "    tsne = TSNE(n_components=2, random_state=42, perplexity=30)\n",
        "    embeddings_tsne = tsne.fit_transform(embeddings_pca)\n",
        "\n",
        "    # Plot with color-coding based on class\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    scatter = plt.scatter(embeddings_tsne[:, 0], embeddings_tsne[:, 1], c=labels, cmap='tab20', alpha=0.7)\n",
        "    plt.colorbar(scatter, label='Class')\n",
        "    plt.title('t-SNE Visualization of Learned Embeddings (CLS tokens)')\n",
        "    plt.xlabel('t-SNE Dimension 1')\n",
        "    plt.ylabel('t-SNE Dimension 2')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(save_path, dpi=300)\n",
        "    plt.close()\n",
        "    print(f\"Embedding visualization saved to {save_path}\")\n",
        "\n",
        "# Visualize attention maps for a sample image\n",
        "def visualize_attention(model, dataloader, device, layer_idx=5, head_idx=0, save_path='attention_visualization.png'):\n",
        "    model.eval()\n",
        "\n",
        "    # Register a hook to get attention maps\n",
        "    attention_maps = []\n",
        "\n",
        "    def hook_fn(module, input, output):\n",
        "        # Output contains attention matrix of shape [B, num_heads, seq_len, seq_len]\n",
        "        attention_maps.append(output)\n",
        "\n",
        "    # Register hook to the specified attention layer\n",
        "    hook = model.blocks[layer_idx].attn.register_forward_hook(hook_fn)\n",
        "\n",
        "    # Get a single image\n",
        "    images, _ = next(iter(dataloader))\n",
        "    img = images[0:1].to(device)  # Just take the first image\n",
        "\n",
        "    # Forward pass\n",
        "    with torch.no_grad():\n",
        "        _, mask, _ = model.forward_encoder(img, mask_ratio=0.75)\n",
        "\n",
        "    # Remove hook\n",
        "    hook.remove()\n",
        "\n",
        "    if len(attention_maps) == 0:\n",
        "        print(\"No attention maps were captured!\")\n",
        "        return\n",
        "\n",
        "    # Get attention map for the specified head\n",
        "    # Shape: [B, num_heads, seq_len, seq_len]\n",
        "    attn_map = attention_maps[0][0, head_idx].cpu().numpy()\n",
        "\n",
        "    # We're interested in attention from CLS token to all other tokens\n",
        "    cls_attention = attn_map[0, 1:]  # Remove CLS to CLS attention\n",
        "\n",
        "    # Reshape attention map to match image patches\n",
        "    patch_size = model.patch_size\n",
        "    num_patches = int(np.sqrt(len(cls_attention)))\n",
        "    attention_map = cls_attention.reshape(num_patches, num_patches)\n",
        "\n",
        "    # Plot\n",
        "    fig, axs = plt.subplots(1, 3, figsize=(15, 5))\n",
        "\n",
        "    # Original image\n",
        "    img_np = images[0].permute(1, 2, 0).numpy()\n",
        "    mean = np.array([0.5071, 0.4867, 0.4408])\n",
        "    std = np.array([0.2675, 0.2565, 0.2761])\n",
        "    img_np = img_np * std + mean\n",
        "    img_np = np.clip(img_np, 0, 1)\n",
        "    axs[0].imshow(img_np)\n",
        "    axs[0].set_title(\"Original Image\")\n",
        "    axs[0].axis('off')\n",
        "\n",
        "    # Mask visualization\n",
        "    mask_vis = mask[0].reshape(num_patches, num_patches).cpu().numpy()\n",
        "    axs[1].imshow(mask_vis, cmap='gray')\n",
        "    axs[1].set_title(\"Mask (white = masked)\")\n",
        "    axs[1].axis('off')\n",
        "\n",
        "    # Attention map\n",
        "    im = axs[2].imshow(attention_map, cmap='viridis')\n",
        "    axs[2].set_title(f\"Attention from CLS token (Layer {layer_idx}, Head {head_idx})\")\n",
        "    axs[2].axis('off')\n",
        "    plt.colorbar(im, ax=axs[2])\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(save_path, dpi=300)\n",
        "    plt.close()\n",
        "    print(f\"Attention visualization saved to {save_path}\")\n",
        "\n",
        "# Generate a comprehensive model evaluation report\n",
        "def evaluate_model(model, testloader, device, save_dir='./vis_results/'):\n",
        "    import os\n",
        "\n",
        "    # Create directory if it doesn't exist\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "    # Load the best model\n",
        "    model, best_epoch, best_loss = load_best_model(model)\n",
        "    model = model.to(device)\n",
        "\n",
        "    # 1. Evaluate reconstruction on multiple batches\n",
        "    print(\"\\n=== Reconstruction Evaluation ===\")\n",
        "    mse_values = []\n",
        "    l1_values = []\n",
        "    psnr_values = []\n",
        "\n",
        "    for i in range(3):  # Evaluate on 3 different batches\n",
        "        print(f\"\\nBatch {i+1}:\")\n",
        "        mse, l1, psnr = visualize_reconstruction_enhanced(\n",
        "            model, testloader, device,\n",
        "            num_images=6,\n",
        "            save_path=f\"{save_dir}reconstruction_batch{i+1}.png\"\n",
        "        )\n",
        "        mse_values.append(mse)\n",
        "        l1_values.append(l1)\n",
        "        psnr_values.append(psnr)\n",
        "\n",
        "    # Print average metrics\n",
        "    print(\"\\nAverage reconstruction metrics:\")\n",
        "    print(f\"MSE: {np.mean(mse_values):.4f} ± {np.std(mse_values):.4f}\")\n",
        "    print(f\"L1: {np.mean(l1_values):.4f} ± {np.std(l1_values):.4f}\")\n",
        "    print(f\"PSNR: {np.mean(psnr_values):.2f} dB ± {np.std(psnr_values):.2f}\")\n",
        "\n",
        "    # 2. Visualize embeddings from the trained model\n",
        "    print(\"\\n=== Embedding Visualization ===\")\n",
        "    visualize_embeddings(model, testloader, device, num_samples=500, save_path=f\"{save_dir}tsne_embeddings.png\")\n",
        "\n",
        "    # 3. Visualize attention maps from different layers and heads\n",
        "    print(\"\\n=== Attention Visualization ===\")\n",
        "    for layer in [1, 3, 5]:  # Early, middle, late layers\n",
        "        for head in [0, 2, 5]:  # Different attention heads\n",
        "            visualize_attention(\n",
        "                model, testloader, device,\n",
        "                layer_idx=layer, head_idx=head,\n",
        "                save_path=f\"{save_dir}attention_layer{layer}_head{head}.png\"\n",
        "            )\n",
        "\n",
        "    # 4. Create a comparison across different mask ratios\n",
        "    print(\"\\n=== Mask Ratio Comparison ===\")\n",
        "    mask_ratios = [0.3, 0.5, 0.75, 0.9]\n",
        "    compare_mask_ratios(model, testloader, device, mask_ratios, save_path=f\"{save_dir}mask_ratio_comparison.png\")\n",
        "\n",
        "    print(f\"\\nAll visualizations saved to {save_dir}\")\n",
        "\n",
        "# Compare reconstructions with different mask ratios\n",
        "def compare_mask_ratios(model, dataloader, device, mask_ratios, save_path='mask_ratio_comparison.png'):\n",
        "    model.eval()\n",
        "\n",
        "    # Get a batch of images\n",
        "    images, _ = next(iter(dataloader))\n",
        "    sample_images = images[:4].to(device)  # Use 4 sample images\n",
        "\n",
        "    # Store reconstructions for each mask ratio\n",
        "    all_reconstructions = []\n",
        "    all_masks = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for ratio in mask_ratios:\n",
        "            reconstructed, mask = model(sample_images, mask_ratio=ratio)\n",
        "            all_reconstructions.append(reconstructed.cpu())\n",
        "\n",
        "            # Reshape mask for visualization\n",
        "            mask_vis = mask.unsqueeze(-1).repeat(1, 1, 16).reshape(mask.shape[0], 32, 32).unsqueeze(1)\n",
        "            mask_vis = mask_vis.repeat(1, 3, 1, 1).cpu()\n",
        "            all_masks.append(mask_vis)\n",
        "\n",
        "    # Denormalize images\n",
        "    mean = torch.tensor([0.5071, 0.4867, 0.4408]).view(1, 3, 1, 1)\n",
        "    std = torch.tensor([0.2675, 0.2565, 0.2761]).view(1, 3, 1, 1)\n",
        "\n",
        "    images = images[:4].cpu() * std + mean\n",
        "    for i in range(len(all_reconstructions)):\n",
        "        all_reconstructions[i] = all_reconstructions[i] * std + mean\n",
        "\n",
        "    # Clip values to [0, 1] range\n",
        "    images = torch.clamp(images, 0, 1)\n",
        "    for i in range(len(all_reconstructions)):\n",
        "        all_reconstructions[i] = torch.clamp(all_reconstructions[i], 0, 1)\n",
        "\n",
        "    # Plot\n",
        "    num_rows = len(mask_ratios) * 2 + 1  # Original images + (mask and reconstruction for each ratio)\n",
        "    num_cols = len(sample_images)\n",
        "    fig, axs = plt.subplots(num_rows, num_cols, figsize=(num_cols*2.5, num_rows*2))\n",
        "\n",
        "    # Plot original images on the first row\n",
        "    for i in range(num_cols):\n",
        "        axs[0, i].imshow(images[i].permute(1, 2, 0))\n",
        "        axs[0, i].set_title(\"Original\", fontsize=10)\n",
        "        axs[0, i].axis('off')\n",
        "\n",
        "    # Plot masks and reconstructions for each mask ratio\n",
        "    for r, ratio in enumerate(mask_ratios):\n",
        "        row_offset = r * 2 + 1\n",
        "\n",
        "        # Masks\n",
        "        for i in range(num_cols):\n",
        "            axs[row_offset, i].imshow(1 - all_masks[r][i].permute(1, 2, 0), cmap='gray')\n",
        "            axs[row_offset, i].set_title(f\"Mask ({ratio*100:.0f}% masked)\", fontsize=10)\n",
        "            axs[row_offset, i].axis('off')\n",
        "\n",
        "        # Reconstructions\n",
        "        for i in range(num_cols):\n",
        "            axs[row_offset+1, i].imshow(all_reconstructions[r][i].permute(1, 2, 0))\n",
        "            axs[row_offset+1, i].set_title(f\"Reconstructed\", fontsize=10)\n",
        "            axs[row_offset+1, i].axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(save_path, dpi=300)\n",
        "    plt.close()\n",
        "    print(f\"Mask ratio comparison saved to {save_path}\")\n",
        "\n",
        "# Call the main evaluation function to generate all visualizations\n",
        "if __name__ == \"__main__\":\n",
        "    # You would typically import your model and dataloader here\n",
        "    # For now, we assume they're already defined\n",
        "\n",
        "    # Create directory for visualizations\n",
        "    import os\n",
        "    os.makedirs(\"./vis_results/\", exist_ok=True)\n",
        "\n",
        "    # Load training losses from a file or variable\n",
        "    # If you saved losses during training\n",
        "    try:\n",
        "        # Try to load losses from previous run\n",
        "        losses = torch.load('training_losses.pth')\n",
        "    except:\n",
        "        # If not available, just create dummy data for demonstration\n",
        "        losses = [0.5 - 0.3 * np.exp(-0.1 * i) + 0.05 * np.random.randn() for i in range(30)]\n",
        "\n",
        "    # Plot training loss\n",
        "    plot_training_loss(losses, save_path=\"./vis_results/training_loss_detailed.png\")\n",
        "\n",
        "    # Evaluate model and generate all visualizations\n",
        "    evaluate_model(model, testloader, device, save_dir=\"./vis_results/\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 918
        },
        "id": "iwQkN02pVyqg",
        "outputId": "2e7c29da-a960-41c3-cdb0-02fa7106707a"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss plot saved to ./vis_results/training_loss_detailed.png\n",
            "Loaded model from epoch 14 with loss: 0.0828\n",
            "\n",
            "=== Reconstruction Evaluation ===\n",
            "\n",
            "Batch 1:\n",
            "Reconstruction visualization saved to ./vis_results/reconstruction_batch1.png\n",
            "Reconstruction metrics on sample:\n",
            "MSE: 0.0001\n",
            "L1: 0.0086\n",
            "PSNR: 38.71 dB\n",
            "\n",
            "Batch 2:\n",
            "Reconstruction visualization saved to ./vis_results/reconstruction_batch2.png\n",
            "Reconstruction metrics on sample:\n",
            "MSE: 0.0001\n",
            "L1: 0.0086\n",
            "PSNR: 38.71 dB\n",
            "\n",
            "Batch 3:\n",
            "Reconstruction visualization saved to ./vis_results/reconstruction_batch3.png\n",
            "Reconstruction metrics on sample:\n",
            "MSE: 0.0001\n",
            "L1: 0.0086\n",
            "PSNR: 38.71 dB\n",
            "\n",
            "Average reconstruction metrics:\n",
            "MSE: 0.0001 ± 0.0000\n",
            "L1: 0.0086 ± 0.0000\n",
            "PSNR: 38.71 dB ± 0.00\n",
            "\n",
            "=== Embedding Visualization ===\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "not enough values to unpack (expected 3, got 2)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-29-238295ea0e83>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    385\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m     \u001b[0;31m# Evaluate model and generate all visualizations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 387\u001b[0;31m     \u001b[0mevaluate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtestloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"./vis_results/\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-29-238295ea0e83>\u001b[0m in \u001b[0;36mevaluate_model\u001b[0;34m(model, testloader, device, save_dir)\u001b[0m\n\u001b[1;32m    277\u001b[0m     \u001b[0;31m# 2. Visualize embeddings from the trained model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n=== Embedding Visualization ===\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 279\u001b[0;31m     \u001b[0mvisualize_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtestloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_samples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mf\"{save_dir}tsne_embeddings.png\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    280\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m     \u001b[0;31m# 3. Visualize attention maps from different layers and heads\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-29-238295ea0e83>\u001b[0m in \u001b[0;36mvisualize_embeddings\u001b[0;34m(model, dataloader, device, num_samples, save_path)\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m             \u001b[0;31m# Get embeddings (CLS token after encoder)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m             \u001b[0mlatent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_encoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask_ratio\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# No masking for this visualization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m             \u001b[0mcls_token\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlatent\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# CLS token\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 3, got 2)"
          ]
        }
      ]
    }
  ]
}